{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 38)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('credit-card-full.csv')\n",
    "sex = pd.get_dummies(df['SEX'], prefix = 'SEX')\n",
    "education = pd.get_dummies(df['EDUCATION'], prefix = 'EDUCATION')\n",
    "marriage = pd.get_dummies(df['MARRIAGE'], prefix = 'MARRIAGE')\n",
    "df2 = pd.concat([df, sex, education, marriage], axis = 1)\n",
    "X = df2[['LIMIT_BAL', 'AGE', 'PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5',\n",
    "       'PAY_6', 'BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4',\n",
    "       'BILL_AMT5', 'BILL_AMT6', 'PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3',\n",
    "       'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6', 'SEX_2', 'EDUCATION_1',\n",
    "       'EDUCATION_2', 'EDUCATION_3', 'EDUCATION_4', 'EDUCATION_5',\n",
    "       'EDUCATION_6', 'MARRIAGE_1', 'MARRIAGE_2', 'MARRIAGE_3']]\n",
    "y = df2['default payment next month']\n",
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters and Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting a Logistic Regression parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of original variable names from the training DataFrame\n",
    "original_variables = X_train.columns\n",
    "\n",
    "# Extract the coefficients of the logistic regression estimator\n",
    "model_coefficients = log_reg_clf.coef_[0]\n",
    "\n",
    "# Create a dataframe of the variables and coefficients & print it out\n",
    "coefficient_df = pd.DataFrame({\"Variable\" : original_variables, \"Coefficient\": model_coefficients})\n",
    "print(coefficient_df)\n",
    "\n",
    "# Print out the top 3 positive variables\n",
    "top_three_df = coefficient_df.sort_values(by='Coefficient', axis=0, ascending=False)[0:3]\n",
    "print(top_three_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting a Random Forest parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the 7th tree from the random forest\n",
    "chosen_tree = rf_clf.estimators_[6]\n",
    "\n",
    "# visualize the graph\n",
    "imgplot = plt.imshow(tree_viz)\n",
    "plt.show()\n",
    "\n",
    "# Extract the parameters and level of the top node\n",
    "split_column = chosen_tree.tree_.feature[0]\n",
    "split_column_name = X_train.columns[split_column]\n",
    "split_value = chosen_tree.tree_.threshold[0]\n",
    "\n",
    "# Print out the feature and level\n",
    "print(\"This node split on feature {}, at a value of {}\".format(split_column_name, split_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Random Forest Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the old estimator, notice which hyperparameter is badly set\n",
    "print(rf_clf_old)\n",
    "\n",
    "# Get confusion matrix & accuracy for the old rf_model\n",
    "print(\"Confustion Matrix: \\n\\n {} \\n Accuracy Score: \\n\\n {}\".format(confusion_matrix(y_test, rf_old_predictions),  accuracy_score(y_test, rf_old_predictions))) \n",
    "\n",
    "# Create a new random forest classifier with better hyperparamaters\n",
    "rf_clf_new = RandomForestClassifier(n_estimators=500)\n",
    "\n",
    "# Fit this to the data and obtain predictions\n",
    "rf_new_predictions = rf_clf_new.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "# Assess the new model\n",
    "print(\"Confustion Matrix: \\n\\n\", confusion_matrix(y_test, rf_new_predictions))\n",
    "print(\"Accuracy Score: \\n\\n\", accuracy_score(y_test, rf_new_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters of KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a knn estimator for each value of n_neighbours\n",
    "knn_5 = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_10 = KNeighborsClassifier(n_neighbors=10)\n",
    "knn_20 = KNeighborsClassifier(n_neighbors=20)\n",
    "\n",
    "# Fit each to the training data & produce predictions\n",
    "knn_5_predictions = knn_5.fit(X_train, y_train).predict(X_test)\n",
    "knn_10_predictions = knn_10.fit(X_train, y_train).predict(X_test)\n",
    "knn_20_predictions = knn_20.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "# Get an accuracy score for each of the models\n",
    "knn_5_accuracy = accuracy_score(y_test, knn_5_predictions)\n",
    "knn_10_accuracy = accuracy_score(y_test, knn_10_predictions)\n",
    "knn_20_accuracy = accuracy_score(y_test, knn_20_predictions)\n",
    "print(\"The accuracy of 5, 10, 20 neighbours was {}, {}, {}\".format(knn_5_accuracy, knn_10_accuracy, knn_20_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automating Hyperparameter Choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the learning rates & results storage\n",
    "learning_rates = [0.001, 0.01, 0.05, 0.1, 0.2, 0.5]\n",
    "results_list = []\n",
    "\n",
    "# Create the for loop to evaluate model predictions for each learning rate\n",
    "for learning_rate in learning_rates:\n",
    "    model = GradientBoostingClassifier(learning_rate=learning_rate)\n",
    "    predictions = model.fit(X_train, y_train).predict(X_test)\n",
    "    # Save the learning rate and accuracy score\n",
    "    results_list.append([learning_rate, accuracy_score(y_test, predictions)])\n",
    "\n",
    "# Gather everything into a DataFrame\n",
    "results_df = pd.DataFrame(results_list, columns=['learning_rate', 'accuracy'])\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the learning rates & accuracies list\n",
    "learn_rates = np.linspace(0.01, 2, num=30)\n",
    "accuracies = []\n",
    "\n",
    "# Create the for loop\n",
    "for learn_rate in learn_rates:\n",
    "  \t# Create the model, predictions & save the accuracies as before\n",
    "    model = GradientBoostingClassifier(learning_rate=learn_rate)\n",
    "    predictions = model.fit(X_train, y_train).predict(X_test)\n",
    "    accuracies.append(accuracy_score(y_test, predictions))\n",
    "\n",
    "# Plot results    \n",
    "plt.plot(learn_rates, accuracies)\n",
    "plt.gca().set(xlabel='learning_rate', ylabel='Accuracy', title='Accuracy for different learning_rates')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Grid Search functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the function\n",
    "def gbm_grid_search(learn_rate, max_depth):\n",
    "\n",
    "\t# Create the model\n",
    "    model = GradientBoostingClassifier(learning_rate=learn_rate, max_depth= max_depth)\n",
    "    \n",
    "    # Use the model to make predictions\n",
    "    predictions = model.fit(X_train, y_train).predict(X_test)\n",
    "    \n",
    "    # Return the hyperparameters and score\n",
    "    return([learn_rate, max_depth, accuracy_score(y_test, predictions)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iteratively tune multiple hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the relevant lists\n",
    "results_list = []\n",
    "learn_rate_list = [0.01, 0.1, 0.5]\n",
    "max_depth_list =[2, 4, 6]\n",
    "\n",
    "# Create the for loop\n",
    "for learn_rate in learn_rate_list:\n",
    "    for max_depth in max_depth_list:\n",
    "        results_list.append(gbm_grid_search(learn_rate, max_depth))\n",
    "\n",
    "# Print the results\n",
    "print(results_list)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list = []\n",
    "learn_rate_list = [0.01, 0.1, 0.5]\n",
    "max_depth_list = [2,4,6]\n",
    "\n",
    "# Extend the function input\n",
    "def gbm_grid_search_extended(learn_rate, max_depth, subsample):\n",
    "\n",
    "\t# Extend the model creation section\n",
    "    model = GradientBoostingClassifier(learning_rate=learn_rate, max_depth=max_depth, subsample=subsample)\n",
    "    \n",
    "    predictions = model.fit(X_train, y_train).predict(X_test)\n",
    "    \n",
    "    # Extend the return part\n",
    "    return([learn_rate, max_depth, subsample, accuracy_score(y_test, predictions)])       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list = []\n",
    "\n",
    "# Create the new list to test\n",
    "subsample_list = [0.4 , 0.6]\n",
    "\n",
    "for learn_rate in learn_rate_list:\n",
    "    for max_depth in max_depth_list:\n",
    "    \n",
    "    \t# Extend the for loop\n",
    "        for subsample in subsample_list:\n",
    "        \t\n",
    "            # Extend the results to include the new hyperparameter\n",
    "            results_list.append(gbm_grid_search_extended(learn_rate, max_depth, subsample))\n",
    "            \n",
    "# Print results\n",
    "print(results_list)            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearchCV with Scikit Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Random Forest Classifier with specified criterion\n",
    "rf_class = RandomForestClassifier(criterion = 'entropy')\n",
    "\n",
    "# Create the parameter grid\n",
    "param_grid = {'max_depth': [2, 4, 8, 15], 'max_features':['auto', 'sqrt']} \n",
    "\n",
    "# Create a GridSearchCV object\n",
    "grid_rf_class = GridSearchCV(\n",
    "    estimator=rf_class,\n",
    "    param_grid=param_grid,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=4,\n",
    "    cv=5,\n",
    "    refit=True, return_train_score=True)\n",
    "print(grid_rf_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the grid search results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the cv_results property into a dataframe & print it out\n",
    "cv_results_df = pd.DataFrame(grid_rf_class.cv_results_)\n",
    "print(cv_results_df)\n",
    "\n",
    "# Get and show the column with dictionaries of the hyperparameters used\n",
    "column = cv_results_df.loc[:, [\"params\"]]\n",
    "print(column)\n",
    "\n",
    "# Get and show the row that had the best mean test score\n",
    "best_row = cv_results_df[cv_results_df[\"rank_test_score\"] == 1]\n",
    "print(best_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing the best results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the ROC_AUC score from the best grid search square\n",
    "best_score = grid_rf_class.best_score_\n",
    "print(best_score)\n",
    "\n",
    "# Recreate the best_row variable\n",
    "best_row = cv_results_df.loc[[grid_rf_class.best_index_]]\n",
    "print(best_row)\n",
    "\n",
    "# Get the n_estimators from the best grid search square\n",
    "best_n_estimators = grid_rf_class.best_params_[\"n_estimators\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the best results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See what type of object the best_estimator_ property is\n",
    "print(type(grid_rf_class.best_estimator_))\n",
    "\n",
    "# Create an array of predictions directly using the best_estimator_ property\n",
    "predictions = grid_rf_class.best_estimator_.predict(X_test)\n",
    "\n",
    "# Take a look to confirm it worked, this should be an array of 1's and 0's\n",
    "print(predictions[0:5])\n",
    "\n",
    "# Now create a confusion matrix \n",
    "print(\"Confustion Matrix \\n\", confusion_matrix(y_test, predictions))\n",
    "\n",
    "# Get the ROC-AUC score\n",
    "print(\"ROC-AUC Score \\n\", roc_auc_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomly Sample Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of values for the learning_rate hyperparameter\n",
    "learn_rate_list = np.linspace(0.01,1.5,200)\n",
    "\n",
    "# Create a list of values for the min_samples_leaf hyperparameter\n",
    "min_samples_list = list(range(10,41))\n",
    "\n",
    "# Combination list\n",
    "combinations_list = [list(x) for x in product(learn_rate_list, min_samples_list)]\n",
    "\n",
    "# Sample hyperparameter combinations for a random search.\n",
    "random_combinations_index = np.random.choice(range(1, len(combinations_list)+1), 250, replace=False)\n",
    "combinations_random_chosen = [combinations_list[x] for x in random_combinations_index]\n",
    "\n",
    "# Print the result\n",
    "print(combinations_random_chosen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomly Search with Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lists for criterion and max_features\n",
    "criterion_list = [\"gini\", \"entropy\"]\n",
    "max_feature_list = [\"auto\", \"sqrt\", \"log2\", None]\n",
    "\n",
    "# Create a list of values for the max_depth hyperparameter\n",
    "max_depth_list = list(range(3,56))\n",
    "\n",
    "# Combination list\n",
    "combinations_list = [list(x) for x in product(criterion_list, max_feature_list, max_depth_list)]\n",
    "\n",
    "# Sample hyperparameter combinations for a random search\n",
    "combinations_random_chosen = random.sample(combinations_list, 150)\n",
    "\n",
    "# Print the result\n",
    "print(combinations_random_chosen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing a Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm how hyperparameter combinations & print\n",
    "number_combs = len(combinations_list)\n",
    "print(number_combs)\n",
    "\n",
    "# Sample and visualise combinations\n",
    "for x in [50, 500, 1500 ]:\n",
    "    sample_hyperparameters(x)\n",
    "    visualize_search()\n",
    "\n",
    "# Sample all the hyperparameter combinations & visualise\n",
    "sample_hyperparameters(number_combs)\n",
    "visualize_search()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The RandomizedSearchCV Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the parameter grid\n",
    "param_grid = {'learning_rate': np.linspace(0.1, 2, 150), 'min_samples_leaf': list(range(20, 65))} \n",
    "\n",
    "# Create a random search object\n",
    "random_GBM_class = RandomizedSearchCV(\n",
    "    estimator = GradientBoostingClassifier(),\n",
    "    param_distributions = param_grid,n_iter = 10,\n",
    "    scoring='accuracy', n_jobs=4, cv = 5, refit=True, return_train_score = True)\n",
    "\n",
    "# Fit to the training data\n",
    "random_GBM_class.fit(X_train, y_train)\n",
    "\n",
    "# Print the values used for both hyperparameters\n",
    "print(random_GBM_class.cv_results_['param_learning_rate'])\n",
    "print(random_GBM_class.cv_results_['param_min_samples_leaf'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomSearchCV in Scikit Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the parameter grid\n",
    "param_grid = {'max_depth': list(range(5,26)), 'max_features': ['auto' , 'sqrt']} \n",
    "\n",
    "# Create a random search object\n",
    "random_rf_class = RandomizedSearchCV(\n",
    "    estimator = RandomForestClassifier(n_estimators=80),\n",
    "    param_distributions = param_grid, n_iter = 5,\n",
    "    scoring='roc_auc', n_jobs=4, cv = 3, refit=True, return_train_score = True)\n",
    "\n",
    "# Fit to the training data\n",
    "random_rf_class.fit(X_train, y_train)\n",
    "\n",
    "# Print the values used for both hyperparameters\n",
    "print(random_rf_class.cv_results_['param_max_depth'])\n",
    "print(random_rf_class.cv_results_['param_max_features'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid and Random Search Side by Side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample grid coordinates\n",
    "grid_combinations_chosen = combinations_list[0:300]\n",
    "\n",
    "# Randomly pick numbers to index\n",
    "random_combinations_index = np.random.choice(range(1, len(combinations_list)), 300, replace=False)\n",
    "\n",
    "# Use indexes to select a random sample\n",
    "random_combinations_chosen = [combinations_list[x] for x in random_combinations_index]\n",
    "\n",
    "# Call the function to produce the visualization\n",
    "visualize_search(grid_combinations_chosen, random_combinations_chosen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Informed Search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Coarse to Fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm the size of the combinations_list\n",
    "print(len(combinations_list))\n",
    "\n",
    "# Sort the results_df by accuracy and print the top 10 rows\n",
    "print(results_df.sort_values(by='accuracy', ascending=False).head(10))\n",
    "\n",
    "# Confirm which hyperparameters were used in this search\n",
    "print(results_df.columns)\n",
    "\n",
    "# Call visualize_hyperparameter() with each hyperparameter in turn\n",
    "visualize_hyperparameter('max_depth')\n",
    "visualize_hyperparameter('min_samples_leaf')\n",
    "visualize_hyperparameter('learn_rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coarse to Fine Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the provided function to visualize the first results\n",
    "# visualize_first()\n",
    "\n",
    "# Create some combinations lists & combine:\n",
    "max_depth_list = list(range(1,21))\n",
    "learn_rate_list = np.linspace(0.001,1,50)\n",
    "\n",
    "# Call the function to visualize the second results\n",
    "visualize_second()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes Rule in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign probabilities to variables \n",
    "p_unhappy = 0.15\n",
    "p_unhappy_close = 0.35\n",
    "\n",
    "# Probabiliy someone will close\n",
    "p_close = 0.07\n",
    "\n",
    "# Probability unhappy person will close\n",
    "p_close_unhappy = (p_unhappy_close * p_close) / p_unhappy\n",
    "print(p_close_unhappy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Hyperparameter tuning with Hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up space dictionary with specified hyperparameters\n",
    "space = {'max_depth': hp.quniform('max_depth', 2, 10, 2),'learning_rate': hp.uniform('learning_rate', 0.001, 0.9)}\n",
    "\n",
    "# Set up objective function\n",
    "def objective(params):\n",
    "    params = {'max_depth': int(params['max_depth']),'learning_rate': params['learning_rate']}\n",
    "    gbm_clf = GradientBoostingClassifier(n_estimators=100, **params) \n",
    "    best_score = cross_val_score(gbm_clf, X_train, y_train, scoring='accuracy', cv=2, n_jobs=4).mean()\n",
    "    loss = 1 - best_score\n",
    "    return loss\n",
    "\n",
    "# Run the algorithm\n",
    "best = fmin(fn=objective,space=space, max_evals=20, rstate=np.random.RandomState(42), algo=tpe.suggest)\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Genetic Hyperparameter Tuning with TPOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the values outlined to the inputs\n",
    "number_generations = 3\n",
    "population_size = 4\n",
    "offspring_size = 3\n",
    "scoring_function = 'accuracy'\n",
    "# Create the tpot classifier\n",
    "tpot_clf = TPOTClassifier(generations=number_generations, population_size=population_size,\n",
    "                          offspring_size=offspring_size, scoring=scoring_function,\n",
    "                          verbosity=2, random_state=2, cv=2)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "tpot_clf.fit(X_train, y_train)\n",
    "\n",
    "# Score on the test set\n",
    "print(tpot_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysing TPOT's stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the tpot classifier \n",
    "tpot_clf = TPOTClassifier(generations=2, population_size=4, offspring_size=3, scoring='accuracy', cv=2,\n",
    "                          verbosity=2, random_state=42)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "tpot_clf.fit(X_train, y_train)\n",
    "\n",
    "# Score on the test set\n",
    "print(tpot_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the tpot classifier \n",
    "tpot_clf = TPOTClassifier(generations=2, population_size=4, offspring_size=3, scoring='accuracy', cv=2,\n",
    "                          verbosity=2, random_state=122)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "tpot_clf.fit(X_train, y_train)\n",
    "\n",
    "# Score on the test set\n",
    "print(tpot_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the tpot classifier \n",
    "tpot_clf = TPOTClassifier(generations=2, population_size=4, offspring_size=3, scoring='accuracy', cv=2,\n",
    "                          verbosity=2, random_state=99)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "tpot_clf.fit(X_train, y_train)\n",
    "\n",
    "# Score on the test set\n",
    "print(tpot_clf.score(X_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
